## Is "Safe AI" the New Y2K?

The AI safety hype is starting to feel eerily familiar, like the Y2K panic of the late 90s. Back then, governments and corporations spent billions preparing for a digital apocalypse that never arrived. In hindsight, itâ€™s still unclear whether the crisis was truly averted by all that effort, or whether it was just wildly overblown. What "is" clear is that many walked away richer, more influential, and quick to credit themselves for "saving" us.

Fast forward to today: a handful of tech labs and thought leaders are sounding the alarm on AI. Superintelligence, existential risk, AI doom; it's a compelling narrative, especially when you're the one holding the keys to the supposed solution.

But is "AI safety" always about safety? Or is it, at least partly, about controlling the narrative, capturing regulatory influence, and keeping the power centralized? When only a few get to define what "safe" means, it conveniently lines up with their business models and public image.

Meanwhile, real and present issues such as bias, surveillance, labor exploitation, misinformation get less attention. The world is being reshaped by AI systems today, but much of the spotlight remains fixed on hypothetical futures.

What if this isn't about preventing a catastrophe? What if it's about managing a profitable panic?
